---
outline: deep
---

# 检索增强生成（RAG）原理和流程

RAG 是 LLM 中最典型也是最流行的设计模式，其全称是 Retrieval Augmented Generation，可以被翻译成 检索增强生成技术，从标题上也能了解其核心的流程 检索 => 增强 => 生成。

## LLM 的局限性

1. 涌现性的智能，是基于概率产生的 **伪智能** ，而不是底层基于逻辑和推理能力 **真智能**。

2. 对领域知识的欠缺：

   - 对知识的更新慢，训练数据不可能每天更新。
   - 特定领域的知识不了解，例如你要创建一个宠物医疗 chat bot，他本身训练数据集这方面的知识占比肯定是少的，就很容易出现幻想问题，然后瞎回答。

RAG 是针对以上 2 点进行解决的。

## RAG 的原理

RAG 的基本流程就是：

1. 用户输入提问。

2. 检索：根据用户提问对 向量数据库 进行相似性检测，查找与回答用户问题最相关的内容。

3. 增强：根据检索的结果，生成 prompt。 一般都会涉及 “仅依赖下述信息源来回答问题” 这种限制 llm 参考信息源的语句，来减少幻想，让回答更加聚焦。

4. 生成：将增强后的 prompt 传递给 llm，返回数据给用户。

RAG 更底层的逻辑是，也是对待 llm 正确的态度：llm 是逻辑推理引擎，而不是信息引擎。所以，由外挂的向量数据库提供最有效的知识，然后由 llm 根据知识进行推理，提供有价值的回复。

## RAG 的流程

1. 加载数据
   因为想要根据用户的提问进行语意检索，我们需要将数据集放到向量数据库中，所以我们需要将不同的数据源加载进来。这里就涉及到多种数据源，例如 pdf、code、现存数据库、云数据库等等。

2. 切分数据
3. 嵌入（embedding）
   用最简单的词袋（words bag）模型来描述一下最简单的 embedding 过程:
   a. 词袋模型就是最简化的情况，把一篇 句子/文章 中的单词提前出来，就像放到一个袋子里一样，认为单词之间是独立的，并不关心词与词之间的上下文关系。
   b. 假设我们有十篇英语文章，那我们可以把每个文章拆分成单词，并且还原成最初的形势（例如 did、does => do），然后我们统计每个词出现的次数。 我们简化一下假设最后结果就是：

   ```makefile
      第一篇文章:
      apple: 10, phone:12
      第二篇文章:
      apple: 8, android: 10, phone: 18
      第三篇文章:
      banana: 6, juice: 10
   ```

   c. 那我们尝试构建一个向量，也就是一个数组，每个位置有一个值，代表每个单词在这个文章中出现的次数

   ```csharp
   变量
   [apple, banana, phone, android, juice]
   ```

   那每篇文章，都能用一个变量来表示：

   ```ini
   第一篇文章: [10, 0, 12, 0, 0]
   第二篇文章: [8, 0, 18, 10, 0]
   第三篇文章: [0, 6, 0, 0, 10]
   ```

   d. 这样我们就能把一篇文章用一个向量来表示了，然后我们可以用最简单的余弦定理去计算两个向量之间的夹角，以此确定两个向量的距离。
   e. 这样，我们就有了通过向量和向量之间的余弦夹角的，来衡量文章之间相似度的能力

   回到我们 RAG 流程中，我们将切分后的每一个文档块使用 embedding 算法转换成一个向量，存储到向量数据库中（vector store）中。这样，每一个原始数据都有一个对应的向量，可以用来检索。

4. 检索数据
   当所有需要的数据都存储到向量数据库中后，我们就可以把用户的提问通过 embedding 成向量，用这个向量去向量数据库中进行检索，找到相似性最高的几个文档块，返回。
5. 增强 prompt
   在有了跟用户提问最相关的文档块后，我们根据文档块去构建 prompt。 一般格式都类似于

   ```ts
   你是一个 xxx 的聊天机器人，你的任务是根据给定的文档回答用户问题，并且回答时仅根据给定的文档，尽可能回答
   用户问题。如果你不知道，你可以回答“我不知道”。

   这是文档:
   {docs}

   用户的提问是:
   {question}
   ```

6. 生成
   然后就是将组装好的 prompt 传递给 chatbot 进行生成回答。
